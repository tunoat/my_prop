\section{Introduction}
It cannot be neglected that a Deep Learning model is one of a powerhouse algorithm in Natural Language Processing (NLP).
However, it is unwise conduct to blindly add a neural layer after layers without considering the essential needed.
Although, the model delivery above could achieved a convincing result.
The blindingly added layer will result in a model complexity leads to an over-sizing and a high memory consumption.
In this researches, the discussion is main focus on a delivery of a highly accurate model while consider the compactness.
Thus, the work will based on previous published works to figure the profound concept within model development.
From such a concept, we derived a new model which removed unnecessary parts in the model.
The unnecessary parts contain unimportant layers which could be removed while not deteriorate a model and less effective features.
Furthermore, we also mentioned a problem of imbalance labeled data over our 190-classes corpus which prevented the model from learning efficiently with published corpus and a weight formula for a loss function that could overcome such a problem.
There are new features introduced that could improve model progressive learning especially in a name entity detection task in Japanese language such as Filtering characters and Japanese particle types.
Finally, we proposed a model that compact, fast learning, and high accuracy by measuring the training time per epoch, F1-score, and an amount of epochs that model used to converge.
The model is compared solemnly to a baseline model that deliver the highest F1-score recently on various perspectives which mentioned previously.